{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db2b25fb",
   "metadata": {},
   "source": [
    "### QSP Hadamard Approximation\n",
    "\n",
    "**Aryan Bawa**  \n",
    "\n",
    "In this notebook, I modify Pennylane.ai's \"Function Fitting using Quantum Signal Processing\" tutorial [here](https://pennylane.ai/qml/demos/function_fitting_qsp) such that I can approximate the action of a Hadamard gate. There are a few main things I change:\n",
    "\n",
    "1. Outputting the full matrix for the circuit, not just the polynomial $ \\langle 0 | U_{QSP}|0 \\rangle $\n",
    "2. Changing how we measure how accurate the approximation is by using Fidelity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1d3b568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# have to do this because some libraries using OpenMP cause issues with multiple threads\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "import torch\n",
    "import pennylane as qml\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b44f0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining important functions\n",
    "def rotation_mat(a):\n",
    "    \"\"\"Given a fixed value 'a', compute the signal rotation matrix W(a).\n",
    "    (requires -1 <= 'a' <= 1)\n",
    "    \"\"\"\n",
    "    diag = a\n",
    "    off_diag = (1 - a**2) ** (1 / 2) * 1j\n",
    "    W = [[diag, off_diag], [off_diag, diag]]\n",
    "\n",
    "    return W\n",
    "\n",
    "def generate_many_sro(a_vals):\n",
    "    \"\"\"Given a tensor of possible 'a' vals, return a tensor of W(a)\"\"\"\n",
    "    w_array = []\n",
    "    for a in a_vals:\n",
    "        w = rotation_mat(a)\n",
    "        w_array.append(w)\n",
    "\n",
    "    return torch.tensor(w_array, dtype=torch.complex64, requires_grad=False)\n",
    "\n",
    "def target_hadamard():\n",
    "    \"\"\"Return the target Hadamard matrix\"\"\"\n",
    "    return torch.tensor([[1/np.sqrt(2), 1/np.sqrt(2)], \n",
    "                        [1/np.sqrt(2), -1/np.sqrt(2)]], \n",
    "                       dtype=torch.complex64)\n",
    "\n",
    "# fidelity-based loss function for the QSP unitary fitting\n",
    "def fidelity_loss(pred_unitaries, target_op):\n",
    "    batch_size = pred_unitaries.shape[0]\n",
    "    fidelities = []\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        U_pred = pred_unitaries[i]\n",
    "        U_target = target_op\n",
    "        \n",
    "        product = torch.matmul(torch.conj(U_pred).T, U_target)\n",
    "        trace = torch.trace(product)\n",
    "        fidelity = torch.abs(trace)**2 / 4.0\n",
    "        fidelities.append(fidelity)\n",
    "    \n",
    "    avg_fidelity = torch.mean(torch.stack(fidelities))\n",
    "    return 1.0 - avg_fidelity\n",
    "    \n",
    "\n",
    "# below is the QSP circuit itself - we don't change the basis this time since we don't need to use the relaxed third condition for QSP\n",
    "@qml.qnode(qml.device(\"default.qubit\", wires=1))\n",
    "def QSP_circ(phi, W):\n",
    "    for angle in phi[:-1]:\n",
    "        qml.RZ(angle, wires=0)\n",
    "        qml.QubitUnitary(W, wires=0)\n",
    "    qml.RZ(phi[-1], wires=0)\n",
    "    return qml.state()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3a4ec9",
   "metadata": {},
   "source": [
    "The Pennylane article uses machine learning to fit specific $ \\phi $ values such that the polynomial in $ \\langle 0 | U_{QSP}|0 \\rangle $ is approximately equal to some P(a). This is done by creating several classes for fitting and running the model. Starting with the fitting class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "be32b9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_pi = torch.Tensor([math.pi])\n",
    "\n",
    "class QSP_Unitary_Fit(torch.nn.Module):\n",
    "    def __init__(self, degree, random_seed=None):\n",
    "        \"\"\"Given the degree and number of samples, this method randomly\n",
    "        initializes the parameter vector (randomness can be set by random_seed)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if random_seed is None:\n",
    "            self.phi = torch_pi * torch.rand(degree + 1, requires_grad=True)\n",
    "\n",
    "        else:\n",
    "            gen = torch.Generator()\n",
    "            gen.manual_seed(random_seed)\n",
    "            self.phi = torch_pi * torch.rand(degree + 1, requires_grad=True, generator=gen)\n",
    "\n",
    "        self.phi = torch.nn.Parameter(self.phi)\n",
    "        self.num_phi = degree + 1\n",
    "\n",
    "    def forward(self, omega_mats):\n",
    "        unitary_pred = []\n",
    "        \n",
    "        for w in omega_mats:\n",
    "            u_qsp = self.matrix_fn(self.phi, w)\n",
    "            unitary_pred.append(u_qsp)\n",
    "\n",
    "        return torch.stack(unitary_pred, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b4d9a0",
   "metadata": {},
   "source": [
    "Next, the Model_Runner class handles running the optimization, storing the results, determining fidelity between approximate and target states, and plotting this metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "10b74337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned this up and added verbose output with Claude AI's help\n",
    "class Model_Runner:\n",
    "    def __init__(self, degree, num_samples):\n",
    "        self.degree = degree\n",
    "        self.num_samples = num_samples\n",
    "        # generating a values and corresponding W(a) matrices\n",
    "        self.a_vals = torch.linspace(-0.99, 0.99, num_samples)\n",
    "        self.omega_mats = generate_many_sro(self.a_vals)\n",
    "        self.target_op = target_hadamard()\n",
    "\n",
    "        print(f\"Initialized model runner\")\n",
    "\n",
    "    def execute(self, max_iterations=5000, lr=1e-3, random_seed=42, verbose=True):\n",
    "        # starting up the model\n",
    "        self.model = QSP_Unitary_Fit(self.degree, random_seed=random_seed)\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        \n",
    "        # storing the initial for comparison\n",
    "        with torch.no_grad():\n",
    "            self.init_pred = self.model(self.omega_mats)\n",
    "        \n",
    "        # Training tracking\n",
    "        self.losses = []\n",
    "        self.fidelities = []\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nTraining QSP to approximate Hadamard gate...\")\n",
    "            print(f\"Learning rate: {lr}\")\n",
    "            print(\"-\" * 50)\n",
    "        \n",
    "        for t in range(max_iterations):\n",
    "            pred_unitaries = self.model(self.omega_mats)\n",
    "            loss = fidelity_loss(pred_unitaries, self.target_op)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            current_loss = loss.item()\n",
    "            current_fidelity = 1.0 - current_loss\n",
    "            \n",
    "            self.losses.append(current_loss)\n",
    "            self.fidelities.append(current_fidelity)\n",
    "            \n",
    "            if verbose and (t % 500 == 0 or t < 10):\n",
    "                print(f\"Iteration {t:5d}: Loss = {current_loss:.8f}, Avg Fidelity = {current_fidelity:.8f}\")\n",
    "            \n",
    "            # Early stopping for very high fidelity\n",
    "            if current_fidelity > 0.9999:\n",
    "                if verbose:\n",
    "                    print(f\"High fidelity achieved! Stopping at iteration {t}\")\n",
    "                break\n",
    "            \n",
    "            # Early stopping for convergence\n",
    "            if current_loss < 1e-8:\n",
    "                if verbose:\n",
    "                    print(f\"Converged at iteration {t}\")\n",
    "                break\n",
    "        \n",
    "        # Store final results\n",
    "        self.final_loss = current_loss\n",
    "        self.final_fidelity = current_fidelity\n",
    "        self.training_iterations = len(self.losses)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nTraining completed after {self.training_iterations} iterations\")\n",
    "            print(f\"Final loss: {self.final_loss:.8f}\")\n",
    "            print(f\"Final average fidelity: {self.final_fidelity:.8f}\")\n",
    "\n",
    "    def evaluate(self, verbose=True):\n",
    "        \"\"\"\n",
    "        Evaluate the quality of the approximation\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'model'):\n",
    "            raise ValueError(\"Must run execute() before evaluate()\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.final_pred = self.model(self.omega_mats)\n",
    "        \n",
    "        # Compute fidelities and errors for each 'a' value\n",
    "        self.fidelities_per_a = []\n",
    "        self.errors_per_a = []\n",
    "        \n",
    "        for i, a in enumerate(self.a_vals):\n",
    "            U_pred = self.final_pred[i]\n",
    "            \n",
    "            # Fidelity using PennyLane's built-in function\n",
    "            U_pred_np = U_pred.detach().numpy()\n",
    "            target_np = self.target_op.detach().numpy()\n",
    "            fidelity = qml.math.fidelity(U_pred_np, target_np, check_state=False, c_dtype=\"complex128\")\n",
    "            self.fidelities_per_a.append(fidelity.item() if hasattr(fidelity, 'item') else float(fidelity))\n",
    "            \n",
    "            # Frobenius error for additional analysis\n",
    "            diff = U_pred - self.target_op\n",
    "            frobenius_error = torch.sqrt(torch.sum(torch.abs(diff)**2)).item()\n",
    "            self.errors_per_a.append(frobenius_error)\n",
    "        \n",
    "        # Compute statistics\n",
    "        self.avg_fidelity = np.mean(self.fidelities_per_a)\n",
    "        self.min_fidelity = np.min(self.fidelities_per_a)\n",
    "        self.max_fidelity = np.max(self.fidelities_per_a)\n",
    "        self.std_fidelity = np.std(self.fidelities_per_a)\n",
    "        self.avg_error = np.mean(self.errors_per_a)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n=== Hadamard Gate Approximation Quality ===\")\n",
    "            print(f\"Average Fidelity:     {self.avg_fidelity:.8f}\")\n",
    "            print(f\"Minimum Fidelity:     {self.min_fidelity:.8f}\")\n",
    "            print(f\"Maximum Fidelity:     {self.max_fidelity:.8f}\")\n",
    "            print(f\"Fidelity Std Dev:     {self.std_fidelity:.8f}\")\n",
    "            print(f\"Avg Frobenius Error:  {self.avg_error:.8f}\")\n",
    "\n",
    "    def plot_results(self, show=True, figsize=(15, 5)):\n",
    "        \"\"\"\n",
    "        Plot comprehensive training and evaluation results\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'losses'):\n",
    "            raise ValueError(\"Must run execute() before plotting\")\n",
    "        \n",
    "        if not hasattr(self, 'fidelities_per_a'):\n",
    "            self.evaluate(verbose=False)\n",
    "        \n",
    "        plt.figure(figsize=figsize)\n",
    "        \n",
    "        # Plot 1: Training progress\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(self.losses, label='Loss', color='red', alpha=0.7)\n",
    "        plt.plot(self.fidelities, label='Avg Fidelity', color='blue', alpha=0.7)\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Value')\n",
    "        plt.title('Training Progress')\n",
    "        plt.legend()\n",
    "        plt.yscale('log')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Plot 2: Fidelity vs parameter 'a'\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(self.a_vals.numpy(), self.fidelities_per_a, 'o-', markersize=4, color='blue', label='Per-sample fidelity')\n",
    "        plt.axhline(y=1.0, color='green', linestyle='--', alpha=0.5, label='Perfect Fidelity')\n",
    "        plt.axhline(y=self.avg_fidelity, color='red', linestyle='-', alpha=0.7, \n",
    "                    label=f'Average: {self.avg_fidelity:.6f}')\n",
    "        plt.xlabel('Parameter a')\n",
    "        plt.ylabel('Fidelity')\n",
    "        plt.title('Fidelity vs Parameter a')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.ylim([max(0.95, self.min_fidelity - 0.01), 1.002])\n",
    "        \n",
    "        # Plot 3: Error analysis\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(self.a_vals.numpy(), self.errors_per_a, 'o-', markersize=4, color='red', label='Frobenius error')\n",
    "        plt.axhline(y=self.avg_error, color='blue', linestyle='-', alpha=0.7,\n",
    "                    label=f'Average: {self.avg_error:.6f}')\n",
    "        plt.xlabel('Parameter a')\n",
    "        plt.ylabel('Frobenius Error')\n",
    "        plt.title('Approximation Error vs Parameter a')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.yscale('log')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if show:\n",
    "            plt.show()\n",
    "\n",
    "    def get_learned_parameters(self):\n",
    "        \"\"\"\n",
    "        Get the learned φ parameters\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'model'):\n",
    "            raise ValueError(\"Must run execute() before getting parameters\")\n",
    "        return self.model.phi.detach().numpy()\n",
    "\n",
    "    def compare_matrices(self, test_a=0.5, verbose=True):\n",
    "        \"\"\"\n",
    "        Compare target vs approximated matrix for a specific 'a' value\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'model'):\n",
    "            raise ValueError(\"Must run execute() before matrix comparison\")\n",
    "        \n",
    "        test_W = torch.tensor(rotation_mat(test_a), dtype=torch.complex64)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            generate_qsp_mat = qml.matrix(QSP_circ, wire_order=[0])\n",
    "            approx_H = generate_qsp_mat(self.model.phi, test_W)\n",
    "            \n",
    "        if verbose:\n",
    "            print(f\"\\n=== Matrix Comparison (a = {test_a}) ===\")\n",
    "            print(\"Target Hadamard:\")\n",
    "            print(self.target_op.numpy())\n",
    "            print(\"\\nApproximated Hadamard:\")\n",
    "            print(approx_H.numpy())\n",
    "            \n",
    "            fidelity = qml.math.fidelity(approx_H.numpy(), self.target_op.numpy(), check_state=False)\n",
    "            print(f\"\\nMatrix fidelity at a={test_a}: {fidelity:.8f}\")\n",
    "        \n",
    "        return approx_H, fidelity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7522a56",
   "metadata": {},
   "source": [
    "Finally, we can actually run the model to approximate a Hadamard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "52da6f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized model runner\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'QSP_Unitary_Fit' object has no attribute 'matrix_fn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[16], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m runner \u001b[38;5;241m=\u001b[39m Model_Runner(degree\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, num_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Execute training\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mrunner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_iterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_seed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Evaluate the results\u001b[39;00m\n\u001b[0;32m      8\u001b[0m runner\u001b[38;5;241m.\u001b[39mevaluate()\n",
      "Cell \u001b[1;32mIn[12], line 20\u001b[0m, in \u001b[0;36mModel_Runner.execute\u001b[1;34m(self, max_iterations, lr, random_seed, verbose)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# storing the initial for comparison\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 20\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minit_pred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43momega_mats\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;66;03m# Training tracking\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlosses \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\bawa\\miniconda3\\envs\\qsvt-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1773\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1771\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1772\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\bawa\\miniconda3\\envs\\qsvt-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1784\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1779\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1780\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1782\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1783\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1784\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1786\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1787\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "Cell \u001b[1;32mIn[14], line 24\u001b[0m, in \u001b[0;36mQSP_Unitary_Fit.forward\u001b[1;34m(self, omega_mats)\u001b[0m\n\u001b[0;32m     21\u001b[0m unitary_pred \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m w \u001b[38;5;129;01min\u001b[39;00m omega_mats:\n\u001b[1;32m---> 24\u001b[0m     u_qsp \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatrix_fn\u001b[49m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mphi, w)\n\u001b[0;32m     25\u001b[0m     unitary_pred\u001b[38;5;241m.\u001b[39mappend(u_qsp)\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mstack(unitary_pred, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\bawa\\miniconda3\\envs\\qsvt-env\\lib\\site-packages\\torch\\nn\\modules\\module.py:1962\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1960\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[0;32m   1961\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[1;32m-> 1962\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[0;32m   1963\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1964\u001b[0m )\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'QSP_Unitary_Fit' object has no attribute 'matrix_fn'"
     ]
    }
   ],
   "source": [
    "# Initialize the runner\n",
    "runner = Model_Runner(degree=5, num_samples=10)\n",
    "\n",
    "# Execute training\n",
    "runner.execute(max_iterations=1000, lr=1e-2, random_seed=42)\n",
    "\n",
    "# Evaluate the results\n",
    "runner.evaluate()\n",
    "\n",
    "# Plot comprehensive results\n",
    "runner.plot_results()\n",
    "\n",
    "# Display learned parameters and additional analysis\n",
    "print(f\"\\n=== Final Results ===\")\n",
    "learned_params = runner.get_learned_parameters()\n",
    "print(f\"Learned φ parameters: {learned_params}\")\n",
    "print(f\"Parameter range: [{learned_params.min():.4f}, {learned_params.max():.4f}]\")\n",
    "print(f\"Training iterations: {runner.training_iterations}\")\n",
    "\n",
    "# Show matrix comparison for a specific 'a' value\n",
    "runner.compare_matrices(test_a=0.5)\n",
    "\n",
    "# Optional: Compare different configurations\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"=== Comparing Different Configurations ===\")\n",
    "\n",
    "configs = [\n",
    "    {\"degree\": 7, \"samples\": 20, \"lr\": 1e-3},\n",
    "    {\"degree\": 9, \"samples\": 25, \"lr\": 5e-4},\n",
    "    {\"degree\": 11, \"samples\": 30, \"lr\": 1e-4}\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, config in enumerate(configs):\n",
    "    print(f\"\\nConfiguration {i+1}: {config}\")\n",
    "    runner_test = Model_Runner(config[\"degree\"], config[\"samples\"])\n",
    "    runner_test.execute(max_iterations=2000, lr=config[\"lr\"], verbose=False)\n",
    "    runner_test.evaluate(verbose=False)\n",
    "    \n",
    "    results.append({\n",
    "        'config': config,\n",
    "        'final_fidelity': runner_test.final_fidelity,\n",
    "        'avg_fidelity': runner_test.avg_fidelity,\n",
    "        'std_fidelity': runner_test.std_fidelity,\n",
    "        'iterations': runner_test.training_iterations\n",
    "    })\n",
    "    \n",
    "    print(f\"  Final training fidelity: {runner_test.final_fidelity:.6f}\")\n",
    "    print(f\"  Average evaluation fidelity: {runner_test.avg_fidelity:.6f}\")\n",
    "    print(f\"  Fidelity std dev: {runner_test.std_fidelity:.6f}\")\n",
    "    print(f\"  Training iterations: {runner_test.training_iterations}\")\n",
    "\n",
    "# Plot configuration comparison\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "degrees = [r['config']['degree'] for r in results]\n",
    "avg_fidelities = [r['avg_fidelity'] for r in results]\n",
    "std_fidelities = [r['std_fidelity'] for r in results]\n",
    "\n",
    "plt.errorbar(degrees, avg_fidelities, yerr=std_fidelities, \n",
    "                marker='o', capsize=5, capthick=2)\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('Average Fidelity')\n",
    "plt.title('Fidelity vs Polynomial Degree')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "samples = [r['config']['samples'] for r in results]\n",
    "iterations = [r['iterations'] for r in results]\n",
    "\n",
    "plt.scatter(samples, iterations, c=degrees, cmap='viridis', s=100)\n",
    "plt.colorbar(label='Degree')\n",
    "plt.xlabel('Number of Samples')\n",
    "plt.ylabel('Training Iterations')\n",
    "plt.title('Training Efficiency')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f37daf9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qsvt-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
