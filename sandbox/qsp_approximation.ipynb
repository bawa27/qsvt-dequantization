{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db2b25fb",
   "metadata": {},
   "source": [
    "### QSP Hadamard Approximation\n",
    "\n",
    "**Aryan Bawa**  \n",
    "\n",
    "In this notebook, I modify Pennylane.ai's \"Function Fitting using Quantum Signal Processing\" tutorial [here](https://pennylane.ai/qml/demos/function_fitting_qsp) such that I can approximate the action of a Hadamard gate. There are a few main things I change:\n",
    "\n",
    "1. Outputting the full matrix for the circuit, not just the polynomial $ \\langle 0 | U_{QSP}|0 \\rangle $\n",
    "2. Changing how we measure how accurate the approximation is by using Fidelity.\n",
    "3.\n",
    "4. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d3b568",
   "metadata": {},
   "outputs": [],
   "source": [
    "# have to do this because some libraries using OpenMP cause issues with multiple threads\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK'] = 'TRUE'\n",
    "\n",
    "import torch\n",
    "import pennylane as qml\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b44f0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining important functions\n",
    "def rotation_mat(a):\n",
    "    \"\"\"Given a fixed value 'a', compute the signal rotation matrix W(a).\n",
    "    (requires -1 <= 'a' <= 1)\n",
    "    \"\"\"\n",
    "    diag = a\n",
    "    off_diag = (1 - a**2) ** (1 / 2) * 1j\n",
    "    W = [[diag, off_diag], [off_diag, diag]]\n",
    "\n",
    "    return W\n",
    "\n",
    "def generate_many_sro(a_vals):\n",
    "    \"\"\"Given a tensor of possible 'a' vals, return a tensor of W(a)\"\"\"\n",
    "    w_array = []\n",
    "    for a in a_vals:\n",
    "        w = rotation_mat(a)\n",
    "        w_array.append(w)\n",
    "\n",
    "    return torch.tensor(w_array, dtype=torch.complex64, requires_grad=False)\n",
    "\n",
    "def target_hadamard():\n",
    "    \"\"\"Return the target Hadamard matrix\"\"\"\n",
    "    return torch.tensor([[1/np.sqrt(2), 1/np.sqrt(2)], \n",
    "                        [1/np.sqrt(2), -1/np.sqrt(2)]], \n",
    "                       dtype=torch.complex64)\n",
    "\n",
    "# fidelity-based loss function for the QSP unitary fitting\n",
    "# I use Pennylane's built-in fidelity function, but I have to convert the tensors to numpy arrays first, and at the end I convert the result back to a torch tensor\n",
    "def fidelity_loss(pred_unitaries, target_op):\n",
    "    batch_size = pred_unitaries.shape[0]\n",
    "    fidelities = []\n",
    "    \n",
    "    # Convert target_op to numpy for PennyLane compatibility\n",
    "    target_np = target_op.detach().numpy() if hasattr(target_op, 'detach') else target_op.numpy()\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "        U_pred = pred_unitaries[i]\n",
    "        U_pred_np = U_pred.detach().numpy() if hasattr(U_pred, 'detach') else U_pred.numpy() # convert to numpy\n",
    "        fidelity = qml.math.fidelity(U_pred_np, target_np, check_state=False, c_dtype=\"complex128\")\n",
    "        fidelities.append(fidelity)\n",
    "    \n",
    "    avg_fidelity = torch.mean(torch.tensor(fidelities, dtype=torch.float32)) # convert to torch tensor\n",
    "    return 1.0 - avg_fidelity\n",
    "    \n",
    "\n",
    "# below is the QSP circuit itself - we don't change the basis this time since we don't need to use the relaxed third condition for QSP\n",
    "@qml.qnode(qml.device(\"default.qubit\", wires=1))\n",
    "def QSP_circ(phi, W):\n",
    "    for angle in phi[:-1]:\n",
    "        qml.RZ(angle, wires=0)\n",
    "        qml.QubitUnitary(W, wires=0)\n",
    "    qml.RZ(phi[-1], wires=0)\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d3a4ec9",
   "metadata": {},
   "source": [
    "The Pennylane article uses machine learning to fit specific $ \\phi $ values such that the polynomial in $ \\langle 0 | U_{QSP}|0 \\rangle $ is approximately equal to some P(a). This is done by creating several classes for fitting and running the model. Starting with the fitting class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be32b9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch_pi = torch.Tensor([math.pi])\n",
    "\n",
    "class QSP_Unitary_Fit(torch.nn.Module):\n",
    "    def __init__(self, degree, random_seed=None):\n",
    "        \"\"\"Given the degree and number of samples, this method randomly\n",
    "        initializes the parameter vector (randomness can be set by random_seed)\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        if random_seed is None:\n",
    "            self.phi = torch_pi * torch.rand(degree + 1, requires_grad=True)\n",
    "\n",
    "        else:\n",
    "            gen = torch.Generator()\n",
    "            gen.manual_seed(random_seed)\n",
    "            self.phi = torch_pi * torch.rand(degree + 1, requires_grad=True, generator=gen)\n",
    "\n",
    "        self.phi = torch.nn.Parameter(self.phi)\n",
    "        self.num_phi = degree + 1\n",
    "\n",
    "    def forward(self, omega_mats):\n",
    "        \"\"\"PennyLane forward implementation\"\"\"\n",
    "        unitary_pred = []\n",
    "        generate_qsp_mat = qml.matrix(QSP_circ, wire_order=[0])\n",
    "\n",
    "        for w in omega_mats:\n",
    "            u_qsp = generate_qsp_mat(self.phi, w)\n",
    "            unitary_pred.append(u_qsp)\n",
    "\n",
    "        return torch.stack(unitary_pred, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0b4d9a0",
   "metadata": {},
   "source": [
    "Next, the Model_Runner class handles running the optimization, storing the results, determining fidelity between approximate and target states, and plotting this metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10b74337",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cleaned this up and added verbose output with Claude AI's help\n",
    "class Model_Runner:\n",
    "    def __init__(self, degree, num_samples):\n",
    "        self.degree = degree\n",
    "        self.num_samples = num_samples\n",
    "        # generating a values and corresponding W(a) matrices\n",
    "        self.a_vals = torch.linspace(-0.99, 0.99, num_samples)\n",
    "        self.omega_mats = generate_many_sro(self.a_vals)\n",
    "        self.target_op = target_hadamard()\n",
    "\n",
    "        print(f\"Initialized model runner\")\n",
    "\n",
    "    def execute(self, max_iterations=5000, lr=1e-3, random_seed=42, verbose=True):\n",
    "        # starting up the model\n",
    "        self.model = QSP_Unitary_Fit(self.degree, random_seed=random_seed)\n",
    "        optimizer = torch.optim.Adam(self.model.parameters(), lr=lr)\n",
    "        \n",
    "        # storing the initial for comparison\n",
    "        with torch.no_grad():\n",
    "            self.init_pred = self.model(self.omega_mats)\n",
    "        \n",
    "        # Training tracking\n",
    "        self.losses = []\n",
    "        self.fidelities = []\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nTraining QSP to approximate Hadamard gate...\")\n",
    "            print(f\"Learning rate: {lr}\")\n",
    "            print(\"-\" * 50)\n",
    "        \n",
    "        for t in range(max_iterations):\n",
    "            pred_unitaries = self.model(self.omega_mats)\n",
    "            loss = fidelity_loss(pred_unitaries, self.target_op)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            current_loss = loss.item()\n",
    "            current_fidelity = 1.0 - current_loss\n",
    "            \n",
    "            self.losses.append(current_loss)\n",
    "            self.fidelities.append(current_fidelity)\n",
    "            \n",
    "            if verbose and (t % 500 == 0 or t < 10):\n",
    "                print(f\"Iteration {t:5d}: Loss = {current_loss:.8f}, Avg Fidelity = {current_fidelity:.8f}\")\n",
    "            \n",
    "            # Early stopping for very high fidelity\n",
    "            if current_fidelity > 0.9999:\n",
    "                if verbose:\n",
    "                    print(f\"High fidelity achieved! Stopping at iteration {t}\")\n",
    "                break\n",
    "            \n",
    "            # Early stopping for convergence\n",
    "            if current_loss < 1e-8:\n",
    "                if verbose:\n",
    "                    print(f\"Converged at iteration {t}\")\n",
    "                break\n",
    "        \n",
    "        # Store final results\n",
    "        self.final_loss = current_loss\n",
    "        self.final_fidelity = current_fidelity\n",
    "        self.training_iterations = len(self.losses)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\nTraining completed after {self.training_iterations} iterations\")\n",
    "            print(f\"Final loss: {self.final_loss:.8f}\")\n",
    "            print(f\"Final average fidelity: {self.final_fidelity:.8f}\")\n",
    "\n",
    "    def evaluate(self, verbose=True):\n",
    "        \"\"\"\n",
    "        Evaluate the quality of the approximation\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'model'):\n",
    "            raise ValueError(\"Must run execute() before evaluate()\")\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            self.final_pred = self.model(self.omega_mats)\n",
    "        \n",
    "        # Compute fidelities and errors for each 'a' value\n",
    "        self.fidelities_per_a = []\n",
    "        self.errors_per_a = []\n",
    "        \n",
    "        for i, a in enumerate(self.a_vals):\n",
    "            U_pred = self.final_pred[i]\n",
    "            \n",
    "            # Fidelity using PennyLane's built-in function\n",
    "            U_pred_np = U_pred.detach().numpy()\n",
    "            target_np = self.target_op.detach().numpy()\n",
    "            fidelity = qml.math.fidelity(U_pred_np, target_np, check_state=False, c_dtype=\"complex128\")\n",
    "            self.fidelities_per_a.append(fidelity.item() if hasattr(fidelity, 'item') else float(fidelity))\n",
    "            \n",
    "            # Frobenius error for additional analysis\n",
    "            diff = U_pred - self.target_op\n",
    "            frobenius_error = torch.sqrt(torch.sum(torch.abs(diff)**2)).item()\n",
    "            self.errors_per_a.append(frobenius_error)\n",
    "        \n",
    "        # Compute statistics\n",
    "        self.avg_fidelity = np.mean(self.fidelities_per_a)\n",
    "        self.min_fidelity = np.min(self.fidelities_per_a)\n",
    "        self.max_fidelity = np.max(self.fidelities_per_a)\n",
    "        self.std_fidelity = np.std(self.fidelities_per_a)\n",
    "        self.avg_error = np.mean(self.errors_per_a)\n",
    "        \n",
    "        if verbose:\n",
    "            print(f\"\\n=== Hadamard Gate Approximation Quality ===\")\n",
    "            print(f\"Average Fidelity:     {self.avg_fidelity:.8f}\")\n",
    "            print(f\"Minimum Fidelity:     {self.min_fidelity:.8f}\")\n",
    "            print(f\"Maximum Fidelity:     {self.max_fidelity:.8f}\")\n",
    "            print(f\"Fidelity Std Dev:     {self.std_fidelity:.8f}\")\n",
    "            print(f\"Avg Frobenius Error:  {self.avg_error:.8f}\")\n",
    "\n",
    "    def plot_results(self, show=True, figsize=(15, 5)):\n",
    "        \"\"\"\n",
    "        Plot comprehensive training and evaluation results\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'losses'):\n",
    "            raise ValueError(\"Must run execute() before plotting\")\n",
    "        \n",
    "        if not hasattr(self, 'fidelities_per_a'):\n",
    "            self.evaluate(verbose=False)\n",
    "        \n",
    "        plt.figure(figsize=figsize)\n",
    "        \n",
    "        # Plot 1: Training progress\n",
    "        plt.subplot(1, 3, 1)\n",
    "        plt.plot(self.losses, label='Loss', color='red', alpha=0.7)\n",
    "        plt.plot(self.fidelities, label='Avg Fidelity', color='blue', alpha=0.7)\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Value')\n",
    "        plt.title('Training Progress')\n",
    "        plt.legend()\n",
    "        plt.yscale('log')\n",
    "        plt.grid(True)\n",
    "        \n",
    "        # Plot 2: Fidelity vs parameter 'a'\n",
    "        plt.subplot(1, 3, 2)\n",
    "        plt.plot(self.a_vals.numpy(), self.fidelities_per_a, 'o-', markersize=4, color='blue', label='Per-sample fidelity')\n",
    "        plt.axhline(y=1.0, color='green', linestyle='--', alpha=0.5, label='Perfect Fidelity')\n",
    "        plt.axhline(y=self.avg_fidelity, color='red', linestyle='-', alpha=0.7, \n",
    "                    label=f'Average: {self.avg_fidelity:.6f}')\n",
    "        plt.xlabel('Parameter a')\n",
    "        plt.ylabel('Fidelity')\n",
    "        plt.title('Fidelity vs Parameter a')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.ylim([max(0.95, self.min_fidelity - 0.01), 1.002])\n",
    "        \n",
    "        # Plot 3: Error analysis\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.plot(self.a_vals.numpy(), self.errors_per_a, 'o-', markersize=4, color='red', label='Frobenius error')\n",
    "        plt.axhline(y=self.avg_error, color='blue', linestyle='-', alpha=0.7,\n",
    "                    label=f'Average: {self.avg_error:.6f}')\n",
    "        plt.xlabel('Parameter a')\n",
    "        plt.ylabel('Frobenius Error')\n",
    "        plt.title('Approximation Error vs Parameter a')\n",
    "        plt.legend()\n",
    "        plt.grid(True)\n",
    "        plt.yscale('log')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        \n",
    "        if show:\n",
    "            plt.show()\n",
    "\n",
    "    def get_learned_parameters(self):\n",
    "        \"\"\"\n",
    "        Get the learned φ parameters\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'model'):\n",
    "            raise ValueError(\"Must run execute() before getting parameters\")\n",
    "        return self.model.phi.detach().numpy()\n",
    "\n",
    "    def compare_matrices(self, test_a=0.5, verbose=True):\n",
    "        \"\"\"\n",
    "        Compare target vs approximated matrix for a specific 'a' value\n",
    "        \"\"\"\n",
    "        if not hasattr(self, 'model'):\n",
    "            raise ValueError(\"Must run execute() before matrix comparison\")\n",
    "        \n",
    "        test_W = torch.tensor(rotation_mat(test_a), dtype=torch.complex64)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            generate_qsp_mat = qml.matrix(QSP_circ_unitary, wire_order=[0])\n",
    "            approx_H = generate_qsp_mat(self.model.phi, test_W)\n",
    "            \n",
    "        if verbose:\n",
    "            print(f\"\\n=== Matrix Comparison (a = {test_a}) ===\")\n",
    "            print(\"Target Hadamard:\")\n",
    "            print(self.target_op.numpy())\n",
    "            print(\"\\nApproximated Hadamard:\")\n",
    "            print(approx_H.numpy())\n",
    "            \n",
    "            fidelity = qml.math.fidelity(approx_H.numpy(), self.target_op.numpy(), check_state=False)\n",
    "            print(f\"\\nMatrix fidelity at a={test_a}: {fidelity:.8f}\")\n",
    "        \n",
    "        return approx_H, fidelity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7522a56",
   "metadata": {},
   "source": [
    "Finally, we can actually run the model to approximate a Hadamard:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52da6f71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the runner\n",
    "runner = Model_Runner(degree=9, num_samples=30)\n",
    "\n",
    "# Execute training\n",
    "runner.execute(max_iterations=5000, lr=1e-3, random_seed=42)\n",
    "\n",
    "# Evaluate the results\n",
    "runner.evaluate()\n",
    "\n",
    "# Plot comprehensive results\n",
    "runner.plot_results()\n",
    "\n",
    "# Display learned parameters and additional analysis\n",
    "print(f\"\\n=== Final Results ===\")\n",
    "learned_params = runner.get_learned_parameters()\n",
    "print(f\"Learned φ parameters: {learned_params}\")\n",
    "print(f\"Parameter range: [{learned_params.min():.4f}, {learned_params.max():.4f}]\")\n",
    "print(f\"Training iterations: {runner.training_iterations}\")\n",
    "\n",
    "# Show matrix comparison for a specific 'a' value\n",
    "runner.compare_matrices(test_a=0.5)\n",
    "\n",
    "# Optional: Compare different configurations\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"=== Comparing Different Configurations ===\")\n",
    "\n",
    "configs = [\n",
    "    {\"degree\": 7, \"samples\": 20, \"lr\": 1e-3},\n",
    "    {\"degree\": 9, \"samples\": 25, \"lr\": 5e-4},\n",
    "    {\"degree\": 11, \"samples\": 30, \"lr\": 1e-4}\n",
    "]\n",
    "\n",
    "results = []\n",
    "\n",
    "for i, config in enumerate(configs):\n",
    "    print(f\"\\nConfiguration {i+1}: {config}\")\n",
    "    runner_test = Model_Runner(config[\"degree\"], config[\"samples\"])\n",
    "    runner_test.execute(max_iterations=2000, lr=config[\"lr\"], verbose=False)\n",
    "    runner_test.evaluate(verbose=False)\n",
    "    \n",
    "    results.append({\n",
    "        'config': config,\n",
    "        'final_fidelity': runner_test.final_fidelity,\n",
    "        'avg_fidelity': runner_test.avg_fidelity,\n",
    "        'std_fidelity': runner_test.std_fidelity,\n",
    "        'iterations': runner_test.training_iterations\n",
    "    })\n",
    "    \n",
    "    print(f\"  Final training fidelity: {runner_test.final_fidelity:.6f}\")\n",
    "    print(f\"  Average evaluation fidelity: {runner_test.avg_fidelity:.6f}\")\n",
    "    print(f\"  Fidelity std dev: {runner_test.std_fidelity:.6f}\")\n",
    "    print(f\"  Training iterations: {runner_test.training_iterations}\")\n",
    "\n",
    "# Plot configuration comparison\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "degrees = [r['config']['degree'] for r in results]\n",
    "avg_fidelities = [r['avg_fidelity'] for r in results]\n",
    "std_fidelities = [r['std_fidelity'] for r in results]\n",
    "\n",
    "plt.errorbar(degrees, avg_fidelities, yerr=std_fidelities, \n",
    "                marker='o', capsize=5, capthick=2)\n",
    "plt.xlabel('Polynomial Degree')\n",
    "plt.ylabel('Average Fidelity')\n",
    "plt.title('Fidelity vs Polynomial Degree')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "samples = [r['config']['samples'] for r in results]\n",
    "iterations = [r['iterations'] for r in results]\n",
    "\n",
    "plt.scatter(samples, iterations, c=degrees, cmap='viridis', s=100)\n",
    "plt.colorbar(label='Degree')\n",
    "plt.xlabel('Number of Samples')\n",
    "plt.ylabel('Training Iterations')\n",
    "plt.title('Training Efficiency')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "qsvt-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
